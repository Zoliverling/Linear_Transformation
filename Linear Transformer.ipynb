{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas torch scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Fourier Transform for the linear transformer encoder\n",
    "class FourierTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierTransform, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the 2D Fourier transform to the last two dimensions\n",
    "        return torch.fft.fft2(x).real\n",
    "\n",
    "# Define the Multiplexed Attention mechanism\n",
    "class MultiplexedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiplexedAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.split_heads(self.Wq(q), batch_size)\n",
    "        k = self.split_heads(self.Wk(k), batch_size)\n",
    "        v = self.split_heads(self.Wv(v), batch_size)\n",
    "        \n",
    "        # Scaled dot product attention\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        dk = torch.tensor(self.depth).float()\n",
    "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.dense(output)\n",
    "\n",
    "# Define the Positionwise Feed-Forward Network\n",
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedforward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# Define the Linear Transformer Model\n",
    "class LinearTransformer(nn.Module):\n",
    "    def __init__(self, feature_size, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(LinearTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.fourier_transform = FourierTransform()\n",
    "        self.positional_encoder = nn.Embedding(1000, d_model)  # Customize based on max sequence length\n",
    "        self.layers = nn.ModuleList([PositionwiseFeedforward(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.multiplexed_attn = MultiplexedAttention(d_model, num_heads)\n",
    "        self.output_layer = nn.Linear(d_model, feature_size)  # Adjust depending on your output size\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        batch_size, seq_length, _ = src.size()\n",
    "\n",
    "        # Positional Encoding\n",
    "        positions = torch.arange(seq_length, device=src.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        src = src + self.positional_encoder(positions)\n",
    "\n",
    "        # Fourier Transform\n",
    "        src = self.fourier_transform(src)\n",
    "\n",
    "        # Positionwise Feed-Forward Networks\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        # Multiplexed Attention Decoder\n",
    "        output = self.multiplexed_attn(src, src, src, src_mask)  # Q=K=V for self-attention\n",
    "\n",
    "        # Output layer to convert back to feature size\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "# Model Parameters\n",
    "feature_size = 1  # Assuming we are working with a single feature, like 'Close' price\n",
    "num_layers = 2\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearTransformer(feature_size, num_layers, d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "# Dummy input for testing the model\n",
    "dummy_input = torch.rand(32, 10, feature_size)  # batch_size, seq_length, feature_size\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(output.shape)  # Expected output shape: (32, 10, feature_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('TSLA.csv')  # Make sure to replace this with your actual file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df.dropna(inplace=True)  # Remove NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     -0.011308\n",
       "2     -0.037349\n",
       "3     -0.002483\n",
       "4     -0.002976\n",
       "5      0.012281\n",
       "         ...   \n",
       "245    0.010482\n",
       "246    0.028777\n",
       "247    0.012084\n",
       "248   -0.022722\n",
       "249   -0.003248\n",
       "Name: log_return, Length: 249, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['log_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df['log_return'] = scaler.fit_transform(df['log_return'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a PyTorch Tensor\n",
    "data = torch.FloatTensor(df['log_return'].values).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your sequence length\n",
    "seq_length = 10  # Based on how many days you want to use to predict the next day\n",
    "# Create sequences\n",
    "inout_seq = create_inout_sequences(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into train and test sets\n",
    "train_size = int(len(inout_seq) * 0.80)\n",
    "train_set = inout_seq[:train_size]\n",
    "test_set = inout_seq[train_size:]\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LinearTransformer(feature_size=1, num_layers=2, d_model=64, d_ff = 2048, num_heads=8)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[-3.3746e-02,  6.5838e-01,  1.0908e-01,  1.6324e-01,  1.1754e-01,\n",
      "          2.8083e-01,  1.2565e-01,  6.7744e-03, -1.2281e-01,  2.4526e-01],\n",
      "        [-2.4374e-01,  5.1095e-01,  3.7219e-01,  1.1934e-02,  6.3143e-02,\n",
      "          1.6346e-01,  1.7941e-01,  6.2162e-01,  2.3668e-01,  9.4636e-03],\n",
      "        [ 5.0649e-01,  2.6822e-01,  3.0171e-01,  4.1845e-01,  2.9655e-01,\n",
      "          2.9647e-01,  2.7662e-01,  5.4439e-01,  5.0016e-01,  3.4185e-01],\n",
      "        [ 5.5576e-01,  5.0649e-01,  2.6822e-01,  3.0171e-01,  4.1845e-01,\n",
      "          2.9655e-01,  2.9647e-01,  2.7662e-01,  5.4439e-01,  5.0016e-01],\n",
      "        [-3.5823e-03,  4.5258e-02,  2.3182e-01,  5.7255e-01,  2.3304e-01,\n",
      "          9.7034e-02,  3.2597e-01, -2.0783e-01,  4.0725e-01,  7.8278e-02],\n",
      "        [-5.3927e-03,  7.7431e-01,  2.1985e-01,  2.8543e-01, -1.1235e-01,\n",
      "          4.7067e-01,  1.5542e-01,  8.0404e-01,  1.3721e-01,  1.8754e-01],\n",
      "        [-1.3798e-01, -1.9385e-02,  1.4473e-01, -2.1656e-01,  1.2750e-01,\n",
      "          1.3045e-01,  2.5678e-01, -5.8057e-02,  1.0827e-01, -1.1173e-01],\n",
      "        [ 2.6436e-01,  1.7100e-01,  2.6726e-01,  1.9065e-01, -3.5823e-03,\n",
      "          4.5258e-02,  2.3182e-01,  5.7255e-01,  2.3304e-01,  9.7034e-02],\n",
      "        [ 6.7744e-03, -1.2281e-01,  2.4526e-01,  1.7931e-01, -2.8728e-01,\n",
      "         -7.1925e-01, -1.8664e-01,  1.5064e-01,  3.3071e-01, -2.2750e-02],\n",
      "        [ 3.4962e-05,  8.7276e-03,  2.2271e-01,  5.5576e-01,  5.0649e-01,\n",
      "          2.6822e-01,  3.0171e-01,  4.1845e-01,  2.9655e-01,  2.9647e-01],\n",
      "        [ 2.9280e-01,  7.3851e-01,  2.3082e-01, -4.1691e-02,  7.8916e-02,\n",
      "         -1.0348e-02,  1.5280e-01,  2.1893e-01,  3.3760e-01,  2.5729e-01],\n",
      "        [ 1.1900e-01,  2.6385e-01,  1.4408e-01, -3.5142e-01,  3.4204e-01,\n",
      "          5.1369e-01,  6.7425e-01,  3.4752e-01, -1.9770e-01,  1.7380e-01],\n",
      "        [ 1.5732e-02, -8.9042e-02, -2.3675e-01,  2.2300e-01,  4.3170e-02,\n",
      "          1.4318e-02,  3.6120e-01,  2.8410e-01,  1.9568e-01, -3.3746e-02],\n",
      "        [ 9.7034e-02,  3.2597e-01, -2.0783e-01,  4.0725e-01,  7.8278e-02,\n",
      "          2.8873e-01,  3.1233e-01, -1.3798e-01, -1.9385e-02,  1.4473e-01],\n",
      "        [-1.0841e-01, -1.3797e-01, -1.0766e-01, -5.3927e-03,  7.7431e-01,\n",
      "          2.1985e-01,  2.8543e-01, -1.1235e-01,  4.7067e-01,  1.5542e-01],\n",
      "        [ 1.4473e-01, -2.1656e-01,  1.2750e-01,  1.3045e-01,  2.5678e-01,\n",
      "         -5.8057e-02,  1.0827e-01, -1.1173e-01, -1.8452e-01,  1.8812e-01],\n",
      "        [-1.5271e-01,  1.8759e-01,  1.5732e-02, -8.9042e-02, -2.3675e-01,\n",
      "          2.2300e-01,  4.3170e-02,  1.4318e-02,  3.6120e-01,  2.8410e-01],\n",
      "        [ 2.6822e-01,  3.0171e-01,  4.1845e-01,  2.9655e-01,  2.9647e-01,\n",
      "          2.7662e-01,  5.4439e-01,  5.0016e-01,  3.4185e-01,  4.5677e-01],\n",
      "        [ 1.6346e-01,  1.7941e-01,  6.2162e-01,  2.3668e-01,  9.4636e-03,\n",
      "          1.1482e-01,  3.3131e-01, -6.7088e-02,  6.0358e-02,  1.5594e-01],\n",
      "        [ 2.5729e-01,  4.2623e-01,  2.3686e-01,  8.3737e-02, -7.6207e-01,\n",
      "          4.9140e-02,  4.5011e-01,  2.1339e-02,  1.1572e-01, -1.4797e-01],\n",
      "        [ 5.3802e-01,  5.3602e-02, -1.9176e-03,  1.0056e-01,  2.5305e-02,\n",
      "          2.6436e-01,  1.7100e-01,  2.6726e-01,  1.9065e-01, -3.5823e-03],\n",
      "        [ 4.1845e-01,  2.9655e-01,  2.9647e-01,  2.7662e-01,  5.4439e-01,\n",
      "          5.0016e-01,  3.4185e-01,  4.5677e-01,  8.0781e-02,  1.1607e-01],\n",
      "        [ 1.7100e-01,  2.6726e-01,  1.9065e-01, -3.5823e-03,  4.5258e-02,\n",
      "          2.3182e-01,  5.7255e-01,  2.3304e-01,  9.7034e-02,  3.2597e-01],\n",
      "        [ 1.4318e-02,  3.6120e-01,  2.8410e-01,  1.9568e-01, -3.3746e-02,\n",
      "          6.5838e-01,  1.0908e-01,  1.6324e-01,  1.1754e-01,  2.8083e-01],\n",
      "        [ 3.3131e-01, -6.7088e-02,  6.0358e-02,  1.5594e-01,  5.2960e-01,\n",
      "          3.0018e-01,  3.0841e-01,  5.6677e-01,  3.4962e-05,  8.7276e-03],\n",
      "        [ 4.6544e-02, -1.8451e-01,  1.2485e-01,  1.2047e-01,  2.5584e-01,\n",
      "         -1.5509e-01,  4.0646e-01,  1.0382e-01,  2.4418e-01,  1.6416e-02],\n",
      "        [-2.8911e-01,  3.0196e-01,  3.5730e-01,  6.8463e-01,  2.0556e-01,\n",
      "          1.1900e-01,  2.6385e-01,  1.4408e-01, -3.5142e-01,  3.4204e-01],\n",
      "        [ 1.9065e-01, -3.5823e-03,  4.5258e-02,  2.3182e-01,  5.7255e-01,\n",
      "          2.3304e-01,  9.7034e-02,  3.2597e-01, -2.0783e-01,  4.0725e-01],\n",
      "        [ 1.7058e-01,  5.3802e-01,  5.3602e-02, -1.9176e-03,  1.0056e-01,\n",
      "          2.5305e-02,  2.6436e-01,  1.7100e-01,  2.6726e-01,  1.9065e-01],\n",
      "        [ 1.1607e-01,  3.0632e-01,  6.0837e-01, -3.5148e-01,  3.2126e-01,\n",
      "         -1.2586e-01, -4.0780e-01,  4.7779e-01,  3.5817e-01,  1.9040e-01],\n",
      "        [ 8.4908e-02, -1.2408e-01,  2.6153e-01,  4.9053e-02,  4.0565e-02,\n",
      "         -1.0841e-01, -1.3797e-01, -1.0766e-01, -5.3927e-03,  7.7431e-01],\n",
      "        [ 1.2047e-01,  2.5584e-01, -1.5509e-01,  4.0646e-01,  1.0382e-01,\n",
      "          2.4418e-01,  1.6416e-02, -3.4039e-02, -7.6294e-01,  2.5993e-01],\n",
      "        [-4.1936e-02,  6.2240e-02,  8.4908e-02, -1.2408e-01,  2.6153e-01,\n",
      "          4.9053e-02,  4.0565e-02, -1.0841e-01, -1.3797e-01, -1.0766e-01],\n",
      "        [ 2.1304e-01, -2.8911e-01,  3.0196e-01,  3.5730e-01,  6.8463e-01,\n",
      "          2.0556e-01,  1.1900e-01,  2.6385e-01,  1.4408e-01, -3.5142e-01],\n",
      "        [ 1.3045e-01,  2.5678e-01, -5.8057e-02,  1.0827e-01, -1.1173e-01,\n",
      "         -1.8452e-01,  1.8812e-01, -3.0806e-02, -5.4951e-03,  1.5985e-01],\n",
      "        [ 3.4204e-01,  5.1369e-01,  6.7425e-01,  3.4752e-01, -1.9770e-01,\n",
      "          1.7380e-01,  1.9597e-01,  3.5530e-01, -1.1406e-01,  1.9373e-01],\n",
      "        [-9.2879e-02,  3.2695e-01, -4.1936e-02,  6.2240e-02,  8.4908e-02,\n",
      "         -1.2408e-01,  2.6153e-01,  4.9053e-02,  4.0565e-02, -1.0841e-01],\n",
      "        [-7.1925e-01, -1.8664e-01,  1.5064e-01,  3.3071e-01, -2.2750e-02,\n",
      "         -1.3577e-01,  2.1304e-01, -2.8911e-01,  3.0196e-01,  3.5730e-01],\n",
      "        [ 8.3737e-02, -7.6207e-01,  4.9140e-02,  4.5011e-01,  2.1339e-02,\n",
      "          1.1572e-01, -1.4797e-01,  5.1159e-01,  1.7978e-01, -6.6685e-02],\n",
      "        [ 3.4752e-01, -1.9770e-01,  1.7380e-01,  1.9597e-01,  3.5530e-01,\n",
      "         -1.1406e-01,  1.9373e-01,  1.7058e-01,  5.3802e-01,  5.3602e-02],\n",
      "        [-2.3675e-01,  2.2300e-01,  4.3170e-02,  1.4318e-02,  3.6120e-01,\n",
      "          2.8410e-01,  1.9568e-01, -3.3746e-02,  6.5838e-01,  1.0908e-01],\n",
      "        [ 1.8754e-01, -3.1425e-01,  5.5316e-01, -1.2640e-02,  1.3172e-01,\n",
      "          4.0753e-02,  1.0000e+00, -5.3198e-02,  2.7269e-01,  3.0056e-01],\n",
      "        [ 7.8278e-02,  2.8873e-01,  3.1233e-01, -1.3798e-01, -1.9385e-02,\n",
      "          1.4473e-01, -2.1656e-01,  1.2750e-01,  1.3045e-01,  2.5678e-01],\n",
      "        [ 4.7779e-01,  3.5817e-01,  1.9040e-01,  2.9280e-01,  7.3851e-01,\n",
      "          2.3082e-01, -4.1691e-02,  7.8916e-02, -1.0348e-02,  1.5280e-01],\n",
      "        [ 1.4408e-01, -3.5142e-01,  3.4204e-01,  5.1369e-01,  6.7425e-01,\n",
      "          3.4752e-01, -1.9770e-01,  1.7380e-01,  1.9597e-01,  3.5530e-01],\n",
      "        [ 1.5064e-01,  3.3071e-01, -2.2750e-02, -1.3577e-01,  2.1304e-01,\n",
      "         -2.8911e-01,  3.0196e-01,  3.5730e-01,  6.8463e-01,  2.0556e-01],\n",
      "        [ 5.5316e-01, -1.2640e-02,  1.3172e-01,  4.0753e-02,  1.0000e+00,\n",
      "         -5.3198e-02,  2.7269e-01,  3.0056e-01,  9.3680e-02, -1.5271e-01],\n",
      "        [ 5.6677e-01,  3.4962e-05,  8.7276e-03,  2.2271e-01,  5.5576e-01,\n",
      "          5.0649e-01,  2.6822e-01,  3.0171e-01,  4.1845e-01,  2.9655e-01],\n",
      "        [-2.2750e-02, -1.3577e-01,  2.1304e-01, -2.8911e-01,  3.0196e-01,\n",
      "          3.5730e-01,  6.8463e-01,  2.0556e-01,  1.1900e-01,  2.6385e-01],\n",
      "        [ 3.0196e-01,  3.5730e-01,  6.8463e-01,  2.0556e-01,  1.1900e-01,\n",
      "          2.6385e-01,  1.4408e-01, -3.5142e-01,  3.4204e-01,  5.1369e-01],\n",
      "        [-1.0348e-02,  1.5280e-01,  2.1893e-01,  3.3760e-01,  2.5729e-01,\n",
      "          4.2623e-01,  2.3686e-01,  8.3737e-02, -7.6207e-01,  4.9140e-02],\n",
      "        [ 2.8543e-01, -1.1235e-01,  4.7067e-01,  1.5542e-01,  8.0404e-01,\n",
      "          1.3721e-01,  1.8754e-01, -3.1425e-01,  5.5316e-01, -1.2640e-02],\n",
      "        [ 2.8873e-01,  3.1233e-01, -1.3798e-01, -1.9385e-02,  1.4473e-01,\n",
      "         -2.1656e-01,  1.2750e-01,  1.3045e-01,  2.5678e-01, -5.8057e-02],\n",
      "        [-2.0783e-01,  4.0725e-01,  7.8278e-02,  2.8873e-01,  3.1233e-01,\n",
      "         -1.3798e-01, -1.9385e-02,  1.4473e-01, -2.1656e-01,  1.2750e-01],\n",
      "        [ 1.7941e-01,  6.2162e-01,  2.3668e-01,  9.4636e-03,  1.1482e-01,\n",
      "          3.3131e-01, -6.7088e-02,  6.0358e-02,  1.5594e-01,  5.2960e-01],\n",
      "        [ 5.7255e-01,  2.3304e-01,  9.7034e-02,  3.2597e-01, -2.0783e-01,\n",
      "          4.0725e-01,  7.8278e-02,  2.8873e-01,  3.1233e-01, -1.3798e-01],\n",
      "        [-1.8664e-01,  1.5064e-01,  3.3071e-01, -2.2750e-02, -1.3577e-01,\n",
      "          2.1304e-01, -2.8911e-01,  3.0196e-01,  3.5730e-01,  6.8463e-01],\n",
      "        [ 1.0908e-01,  1.6324e-01,  1.1754e-01,  2.8083e-01,  1.2565e-01,\n",
      "          6.7744e-03, -1.2281e-01,  2.4526e-01,  1.7931e-01, -2.8728e-01],\n",
      "        [ 2.6385e-01,  1.4408e-01, -3.5142e-01,  3.4204e-01,  5.1369e-01,\n",
      "          6.7425e-01,  3.4752e-01, -1.9770e-01,  1.7380e-01,  1.9597e-01],\n",
      "        [-1.2281e-01,  2.4526e-01,  1.7931e-01, -2.8728e-01, -7.1925e-01,\n",
      "         -1.8664e-01,  1.5064e-01,  3.3071e-01, -2.2750e-02, -1.3577e-01],\n",
      "        [ 6.2162e-01,  2.3668e-01,  9.4636e-03,  1.1482e-01,  3.3131e-01,\n",
      "         -6.7088e-02,  6.0358e-02,  1.5594e-01,  5.2960e-01,  3.0018e-01],\n",
      "        [ 1.1572e-01, -1.4797e-01,  5.1159e-01,  1.7978e-01, -6.6685e-02,\n",
      "         -9.2879e-02,  3.2695e-01, -4.1936e-02,  6.2240e-02,  8.4908e-02],\n",
      "        [ 6.5838e-01,  1.0908e-01,  1.6324e-01,  1.1754e-01,  2.8083e-01,\n",
      "          1.2565e-01,  6.7744e-03, -1.2281e-01,  2.4526e-01,  1.7931e-01],\n",
      "        [ 6.3143e-02,  1.6346e-01,  1.7941e-01,  6.2162e-01,  2.3668e-01,\n",
      "          9.4636e-03,  1.1482e-01,  3.3131e-01, -6.7088e-02,  6.0358e-02]]), tensor([[ 0.1793],\n",
      "        [ 0.1148],\n",
      "        [ 0.4568],\n",
      "        [ 0.3418],\n",
      "        [ 0.2887],\n",
      "        [-0.3142],\n",
      "        [-0.1845],\n",
      "        [ 0.3260],\n",
      "        [-0.1358],\n",
      "        [ 0.2766],\n",
      "        [ 0.4262],\n",
      "        [ 0.1960],\n",
      "        [ 0.6584],\n",
      "        [-0.2166],\n",
      "        [ 0.8040],\n",
      "        [-0.0308],\n",
      "        [ 0.1957],\n",
      "        [ 0.0808],\n",
      "        [ 0.5296],\n",
      "        [ 0.5116],\n",
      "        [ 0.0453],\n",
      "        [ 0.3063],\n",
      "        [-0.2078],\n",
      "        [ 0.1256],\n",
      "        [ 0.2227],\n",
      "        [-0.0340],\n",
      "        [ 0.5137],\n",
      "        [ 0.0783],\n",
      "        [-0.0036],\n",
      "        [ 0.2928],\n",
      "        [ 0.2198],\n",
      "        [ 0.0098],\n",
      "        [-0.0054],\n",
      "        [ 0.3420],\n",
      "        [ 0.0040],\n",
      "        [ 0.1706],\n",
      "        [-0.1380],\n",
      "        [ 0.6846],\n",
      "        [-0.0929],\n",
      "        [-0.0019],\n",
      "        [ 0.1632],\n",
      "        [ 0.0937],\n",
      "        [-0.0581],\n",
      "        [ 0.2189],\n",
      "        [-0.1141],\n",
      "        [ 0.1190],\n",
      "        [ 0.1876],\n",
      "        [ 0.2965],\n",
      "        [ 0.1441],\n",
      "        [ 0.6743],\n",
      "        [ 0.4501],\n",
      "        [ 0.1317],\n",
      "        [ 0.1083],\n",
      "        [ 0.1305],\n",
      "        [ 0.3002],\n",
      "        [-0.0194],\n",
      "        [ 0.2056],\n",
      "        [-0.7192],\n",
      "        [ 0.3553],\n",
      "        [ 0.2130],\n",
      "        [ 0.3084],\n",
      "        [-0.1241],\n",
      "        [-0.2873],\n",
      "        [ 0.1559]])]\n",
      "1 [tensor([[ 1.0382e-01,  2.4418e-01,  1.6416e-02, -3.4039e-02, -7.6294e-01,\n",
      "          2.5993e-01,  9.8393e-03,  4.3657e-02, -2.4374e-01,  5.1095e-01],\n",
      "        [ 2.8410e-01,  1.9568e-01, -3.3746e-02,  6.5838e-01,  1.0908e-01,\n",
      "          1.6324e-01,  1.1754e-01,  2.8083e-01,  1.2565e-01,  6.7744e-03],\n",
      "        [ 7.3851e-01,  2.3082e-01, -4.1691e-02,  7.8916e-02, -1.0348e-02,\n",
      "          1.5280e-01,  2.1893e-01,  3.3760e-01,  2.5729e-01,  4.2623e-01],\n",
      "        [ 5.3602e-02, -1.9176e-03,  1.0056e-01,  2.5305e-02,  2.6436e-01,\n",
      "          1.7100e-01,  2.6726e-01,  1.9065e-01, -3.5823e-03,  4.5258e-02],\n",
      "        [ 2.3686e-01,  8.3737e-02, -7.6207e-01,  4.9140e-02,  4.5011e-01,\n",
      "          2.1339e-02,  1.1572e-01, -1.4797e-01,  5.1159e-01,  1.7978e-01],\n",
      "        [-1.5509e-01,  4.0646e-01,  1.0382e-01,  2.4418e-01,  1.6416e-02,\n",
      "         -3.4039e-02, -7.6294e-01,  2.5993e-01,  9.8393e-03,  4.3657e-02],\n",
      "        [-1.3577e-01,  2.1304e-01, -2.8911e-01,  3.0196e-01,  3.5730e-01,\n",
      "          6.8463e-01,  2.0556e-01,  1.1900e-01,  2.6385e-01,  1.4408e-01],\n",
      "        [ 2.3668e-01,  9.4636e-03,  1.1482e-01,  3.3131e-01, -6.7088e-02,\n",
      "          6.0358e-02,  1.5594e-01,  5.2960e-01,  3.0018e-01,  3.0841e-01],\n",
      "        [ 3.2126e-01, -1.2586e-01, -4.0780e-01,  4.7779e-01,  3.5817e-01,\n",
      "          1.9040e-01,  2.9280e-01,  7.3851e-01,  2.3082e-01, -4.1691e-02],\n",
      "        [ 7.7431e-01,  2.1985e-01,  2.8543e-01, -1.1235e-01,  4.7067e-01,\n",
      "          1.5542e-01,  8.0404e-01,  1.3721e-01,  1.8754e-01, -3.1425e-01],\n",
      "        [ 1.2485e-01,  1.2047e-01,  2.5584e-01, -1.5509e-01,  4.0646e-01,\n",
      "          1.0382e-01,  2.4418e-01,  1.6416e-02, -3.4039e-02, -7.6294e-01],\n",
      "        [-3.5142e-01,  3.4204e-01,  5.1369e-01,  6.7425e-01,  3.4752e-01,\n",
      "         -1.9770e-01,  1.7380e-01,  1.9597e-01,  3.5530e-01, -1.1406e-01],\n",
      "        [ 5.1159e-01,  1.7978e-01, -6.6685e-02, -9.2879e-02,  3.2695e-01,\n",
      "         -4.1936e-02,  6.2240e-02,  8.4908e-02, -1.2408e-01,  2.6153e-01],\n",
      "        [-3.4039e-02, -7.6294e-01,  2.5993e-01,  9.8393e-03,  4.3657e-02,\n",
      "         -2.4374e-01,  5.1095e-01,  3.7219e-01,  1.1934e-02,  6.3143e-02],\n",
      "        [ 4.5677e-01,  8.0781e-02,  1.1607e-01,  3.0632e-01,  6.0837e-01,\n",
      "         -3.5148e-01,  3.2126e-01, -1.2586e-01, -4.0780e-01,  4.7779e-01],\n",
      "        [-1.2586e-01, -4.0780e-01,  4.7779e-01,  3.5817e-01,  1.9040e-01,\n",
      "          2.9280e-01,  7.3851e-01,  2.3082e-01, -4.1691e-02,  7.8916e-02],\n",
      "        [ 3.1233e-01, -1.3798e-01, -1.9385e-02,  1.4473e-01, -2.1656e-01,\n",
      "          1.2750e-01,  1.3045e-01,  2.5678e-01, -5.8057e-02,  1.0827e-01],\n",
      "        [ 2.9647e-01,  2.7662e-01,  5.4439e-01,  5.0016e-01,  3.4185e-01,\n",
      "          4.5677e-01,  8.0781e-02,  1.1607e-01,  3.0632e-01,  6.0837e-01],\n",
      "        [ 6.2240e-02,  8.4908e-02, -1.2408e-01,  2.6153e-01,  4.9053e-02,\n",
      "          4.0565e-02, -1.0841e-01, -1.3797e-01, -1.0766e-01, -5.3927e-03],\n",
      "        [ 2.1985e-01,  2.8543e-01, -1.1235e-01,  4.7067e-01,  1.5542e-01,\n",
      "          8.0404e-01,  1.3721e-01,  1.8754e-01, -3.1425e-01,  5.5316e-01],\n",
      "        [ 2.7269e-01,  3.0056e-01,  9.3680e-02, -1.5271e-01,  1.8759e-01,\n",
      "          1.5732e-02, -8.9042e-02, -2.3675e-01,  2.2300e-01,  4.3170e-02],\n",
      "        [ 1.8759e-01,  1.5732e-02, -8.9042e-02, -2.3675e-01,  2.2300e-01,\n",
      "          4.3170e-02,  1.4318e-02,  3.6120e-01,  2.8410e-01,  1.9568e-01],\n",
      "        [ 1.9568e-01, -3.3746e-02,  6.5838e-01,  1.0908e-01,  1.6324e-01,\n",
      "          1.1754e-01,  2.8083e-01,  1.2565e-01,  6.7744e-03, -1.2281e-01],\n",
      "        [ 2.1339e-02,  1.1572e-01, -1.4797e-01,  5.1159e-01,  1.7978e-01,\n",
      "         -6.6685e-02, -9.2879e-02,  3.2695e-01, -4.1936e-02,  6.2240e-02],\n",
      "        [ 8.0781e-02,  1.1607e-01,  3.0632e-01,  6.0837e-01, -3.5148e-01,\n",
      "          3.2126e-01, -1.2586e-01, -4.0780e-01,  4.7779e-01,  3.5817e-01],\n",
      "        [ 4.0565e-02, -1.0841e-01, -1.3797e-01, -1.0766e-01, -5.3927e-03,\n",
      "          7.7431e-01,  2.1985e-01,  2.8543e-01, -1.1235e-01,  4.7067e-01],\n",
      "        [ 1.3172e-01,  4.0753e-02,  1.0000e+00, -5.3198e-02,  2.7269e-01,\n",
      "          3.0056e-01,  9.3680e-02, -1.5271e-01,  1.8759e-01,  1.5732e-02],\n",
      "        [ 1.7931e-01, -2.8728e-01, -7.1925e-01, -1.8664e-01,  1.5064e-01,\n",
      "          3.3071e-01, -2.2750e-02, -1.3577e-01,  2.1304e-01, -2.8911e-01],\n",
      "        [-1.9385e-02,  1.4473e-01, -2.1656e-01,  1.2750e-01,  1.3045e-01,\n",
      "          2.5678e-01, -5.8057e-02,  1.0827e-01, -1.1173e-01, -1.8452e-01],\n",
      "        [ 3.0056e-01,  9.3680e-02, -1.5271e-01,  1.8759e-01,  1.5732e-02,\n",
      "         -8.9042e-02, -2.3675e-01,  2.2300e-01,  4.3170e-02,  1.4318e-02],\n",
      "        [ 1.2750e-01,  1.3045e-01,  2.5678e-01, -5.8057e-02,  1.0827e-01,\n",
      "         -1.1173e-01, -1.8452e-01,  1.8812e-01, -3.0806e-02, -5.4951e-03],\n",
      "        [ 2.5993e-01,  9.8393e-03,  4.3657e-02, -2.4374e-01,  5.1095e-01,\n",
      "          3.7219e-01,  1.1934e-02,  6.3143e-02,  1.6346e-01,  1.7941e-01],\n",
      "        [-5.3198e-02,  2.7269e-01,  3.0056e-01,  9.3680e-02, -1.5271e-01,\n",
      "          1.8759e-01,  1.5732e-02, -8.9042e-02, -2.3675e-01,  2.2300e-01],\n",
      "        [ 1.0000e+00, -5.3198e-02,  2.7269e-01,  3.0056e-01,  9.3680e-02,\n",
      "         -1.5271e-01,  1.8759e-01,  1.5732e-02, -8.9042e-02, -2.3675e-01],\n",
      "        [ 2.2300e-01,  4.3170e-02,  1.4318e-02,  3.6120e-01,  2.8410e-01,\n",
      "          1.9568e-01, -3.3746e-02,  6.5838e-01,  1.0908e-01,  1.6324e-01],\n",
      "        [ 9.3680e-02, -1.5271e-01,  1.8759e-01,  1.5732e-02, -8.9042e-02,\n",
      "         -2.3675e-01,  2.2300e-01,  4.3170e-02,  1.4318e-02,  3.6120e-01],\n",
      "        [-1.1406e-01,  1.9373e-01,  1.7058e-01,  5.3802e-01,  5.3602e-02,\n",
      "         -1.9176e-03,  1.0056e-01,  2.5305e-02,  2.6436e-01,  1.7100e-01],\n",
      "        [ 6.7425e-01,  3.4752e-01, -1.9770e-01,  1.7380e-01,  1.9597e-01,\n",
      "          3.5530e-01, -1.1406e-01,  1.9373e-01,  1.7058e-01,  5.3802e-01],\n",
      "        [ 2.1893e-01,  3.3760e-01,  2.5729e-01,  4.2623e-01,  2.3686e-01,\n",
      "          8.3737e-02, -7.6207e-01,  4.9140e-02,  4.5011e-01,  2.1339e-02],\n",
      "        [ 3.5530e-01, -1.1406e-01,  1.9373e-01,  1.7058e-01,  5.3802e-01,\n",
      "          5.3602e-02, -1.9176e-03,  1.0056e-01,  2.5305e-02,  2.6436e-01],\n",
      "        [ 1.1754e-01,  2.8083e-01,  1.2565e-01,  6.7744e-03, -1.2281e-01,\n",
      "          2.4526e-01,  1.7931e-01, -2.8728e-01, -7.1925e-01, -1.8664e-01],\n",
      "        [ 4.7067e-01,  1.5542e-01,  8.0404e-01,  1.3721e-01,  1.8754e-01,\n",
      "         -3.1425e-01,  5.5316e-01, -1.2640e-02,  1.3172e-01,  4.0753e-02],\n",
      "        [ 9.4636e-03,  1.1482e-01,  3.3131e-01, -6.7088e-02,  6.0358e-02,\n",
      "          1.5594e-01,  5.2960e-01,  3.0018e-01,  3.0841e-01,  5.6677e-01],\n",
      "        [ 3.0632e-01,  6.0837e-01, -3.5148e-01,  3.2126e-01, -1.2586e-01,\n",
      "         -4.0780e-01,  4.7779e-01,  3.5817e-01,  1.9040e-01,  2.9280e-01],\n",
      "        [ 3.0171e-01,  4.1845e-01,  2.9655e-01,  2.9647e-01,  2.7662e-01,\n",
      "          5.4439e-01,  5.0016e-01,  3.4185e-01,  4.5677e-01,  8.0781e-02],\n",
      "        [ 2.3182e-01,  5.7255e-01,  2.3304e-01,  9.7034e-02,  3.2597e-01,\n",
      "         -2.0783e-01,  4.0725e-01,  7.8278e-02,  2.8873e-01,  3.1233e-01],\n",
      "        [-7.6207e-01,  4.9140e-02,  4.5011e-01,  2.1339e-02,  1.1572e-01,\n",
      "         -1.4797e-01,  5.1159e-01,  1.7978e-01, -6.6685e-02, -9.2879e-02],\n",
      "        [ 1.5280e-01,  2.1893e-01,  3.3760e-01,  2.5729e-01,  4.2623e-01,\n",
      "          2.3686e-01,  8.3737e-02, -7.6207e-01,  4.9140e-02,  4.5011e-01],\n",
      "        [ 3.3071e-01, -2.2750e-02, -1.3577e-01,  2.1304e-01, -2.8911e-01,\n",
      "          3.0196e-01,  3.5730e-01,  6.8463e-01,  2.0556e-01,  1.1900e-01],\n",
      "        [ 4.5258e-02,  2.3182e-01,  5.7255e-01,  2.3304e-01,  9.7034e-02,\n",
      "          3.2597e-01, -2.0783e-01,  4.0725e-01,  7.8278e-02,  2.8873e-01],\n",
      "        [ 5.1369e-01,  6.7425e-01,  3.4752e-01, -1.9770e-01,  1.7380e-01,\n",
      "          1.9597e-01,  3.5530e-01, -1.1406e-01,  1.9373e-01,  1.7058e-01],\n",
      "        [-3.5148e-01,  3.2126e-01, -1.2586e-01, -4.0780e-01,  4.7779e-01,\n",
      "          3.5817e-01,  1.9040e-01,  2.9280e-01,  7.3851e-01,  2.3082e-01],\n",
      "        [ 7.8916e-02, -1.0348e-02,  1.5280e-01,  2.1893e-01,  3.3760e-01,\n",
      "          2.5729e-01,  4.2623e-01,  2.3686e-01,  8.3737e-02, -7.6207e-01],\n",
      "        [ 4.3657e-02, -2.4374e-01,  5.1095e-01,  3.7219e-01,  1.1934e-02,\n",
      "          6.3143e-02,  1.6346e-01,  1.7941e-01,  6.2162e-01,  2.3668e-01],\n",
      "        [ 2.5305e-02,  2.6436e-01,  1.7100e-01,  2.6726e-01,  1.9065e-01,\n",
      "         -3.5823e-03,  4.5258e-02,  2.3182e-01,  5.7255e-01,  2.3304e-01],\n",
      "        [ 3.7219e-01,  1.1934e-02,  6.3143e-02,  1.6346e-01,  1.7941e-01,\n",
      "          6.2162e-01,  2.3668e-01,  9.4636e-03,  1.1482e-01,  3.3131e-01],\n",
      "        [ 4.0646e-01,  1.0382e-01,  2.4418e-01,  1.6416e-02, -3.4039e-02,\n",
      "         -7.6294e-01,  2.5993e-01,  9.8393e-03,  4.3657e-02, -2.4374e-01],\n",
      "        [ 3.2597e-01, -2.0783e-01,  4.0725e-01,  7.8278e-02,  2.8873e-01,\n",
      "          3.1233e-01, -1.3798e-01, -1.9385e-02,  1.4473e-01, -2.1656e-01],\n",
      "        [ 3.0018e-01,  3.0841e-01,  5.6677e-01,  3.4962e-05,  8.7276e-03,\n",
      "          2.2271e-01,  5.5576e-01,  5.0649e-01,  2.6822e-01,  3.0171e-01],\n",
      "        [ 2.5584e-01, -1.5509e-01,  4.0646e-01,  1.0382e-01,  2.4418e-01,\n",
      "          1.6416e-02, -3.4039e-02, -7.6294e-01,  2.5993e-01,  9.8393e-03],\n",
      "        [ 4.0725e-01,  7.8278e-02,  2.8873e-01,  3.1233e-01, -1.3798e-01,\n",
      "         -1.9385e-02,  1.4473e-01, -2.1656e-01,  1.2750e-01,  1.3045e-01],\n",
      "        [-2.1656e-01,  1.2750e-01,  1.3045e-01,  2.5678e-01, -5.8057e-02,\n",
      "          1.0827e-01, -1.1173e-01, -1.8452e-01,  1.8812e-01, -3.0806e-02],\n",
      "        [ 1.7978e-01, -6.6685e-02, -9.2879e-02,  3.2695e-01, -4.1936e-02,\n",
      "          6.2240e-02,  8.4908e-02, -1.2408e-01,  2.6153e-01,  4.9053e-02],\n",
      "        [ 1.1934e-02,  6.3143e-02,  1.6346e-01,  1.7941e-01,  6.2162e-01,\n",
      "          2.3668e-01,  9.4636e-03,  1.1482e-01,  3.3131e-01, -6.7088e-02]]), tensor([[ 3.7219e-01],\n",
      "        [-1.2281e-01],\n",
      "        [ 2.3686e-01],\n",
      "        [ 2.3182e-01],\n",
      "        [-6.6685e-02],\n",
      "        [-2.4374e-01],\n",
      "        [-3.5142e-01],\n",
      "        [ 5.6677e-01],\n",
      "        [ 7.8916e-02],\n",
      "        [ 5.5316e-01],\n",
      "        [ 2.5993e-01],\n",
      "        [ 1.9373e-01],\n",
      "        [ 4.9053e-02],\n",
      "        [ 1.6346e-01],\n",
      "        [ 3.5817e-01],\n",
      "        [-1.0348e-02],\n",
      "        [-1.1173e-01],\n",
      "        [-3.5148e-01],\n",
      "        [ 7.7431e-01],\n",
      "        [-1.2640e-02],\n",
      "        [ 1.4318e-02],\n",
      "        [-3.3746e-02],\n",
      "        [ 2.4526e-01],\n",
      "        [ 8.4908e-02],\n",
      "        [ 1.9040e-01],\n",
      "        [ 1.5542e-01],\n",
      "        [-8.9042e-02],\n",
      "        [ 3.0196e-01],\n",
      "        [ 1.8812e-01],\n",
      "        [ 3.6120e-01],\n",
      "        [ 1.5985e-01],\n",
      "        [ 6.2162e-01],\n",
      "        [ 4.3170e-02],\n",
      "        [ 2.2300e-01],\n",
      "        [ 1.1754e-01],\n",
      "        [ 2.8410e-01],\n",
      "        [ 2.6726e-01],\n",
      "        [ 5.3602e-02],\n",
      "        [ 1.1572e-01],\n",
      "        [ 1.7100e-01],\n",
      "        [ 1.5064e-01],\n",
      "        [ 1.0000e+00],\n",
      "        [ 3.4962e-05],\n",
      "        [ 7.3851e-01],\n",
      "        [ 1.1607e-01],\n",
      "        [-1.3798e-01],\n",
      "        [ 3.2695e-01],\n",
      "        [ 2.1339e-02],\n",
      "        [ 2.6385e-01],\n",
      "        [ 3.1233e-01],\n",
      "        [ 5.3802e-01],\n",
      "        [-4.1691e-02],\n",
      "        [ 4.9140e-02],\n",
      "        [ 9.4636e-03],\n",
      "        [ 9.7034e-02],\n",
      "        [-6.7088e-02],\n",
      "        [ 5.1095e-01],\n",
      "        [ 1.2750e-01],\n",
      "        [ 4.1845e-01],\n",
      "        [ 4.3657e-02],\n",
      "        [ 2.5678e-01],\n",
      "        [-5.4951e-03],\n",
      "        [ 4.0565e-02],\n",
      "        [ 6.0358e-02]])]\n",
      "2 [tensor([[ 2.3304e-01,  9.7034e-02,  3.2597e-01, -2.0783e-01,  4.0725e-01,\n",
      "          7.8278e-02,  2.8873e-01,  3.1233e-01, -1.3798e-01, -1.9385e-02],\n",
      "        [ 6.0358e-02,  1.5594e-01,  5.2960e-01,  3.0018e-01,  3.0841e-01,\n",
      "          5.6677e-01,  3.4962e-05,  8.7276e-03,  2.2271e-01,  5.5576e-01],\n",
      "        [ 2.6726e-01,  1.9065e-01, -3.5823e-03,  4.5258e-02,  2.3182e-01,\n",
      "          5.7255e-01,  2.3304e-01,  9.7034e-02,  3.2597e-01, -2.0783e-01],\n",
      "        [ 3.4185e-01,  4.5677e-01,  8.0781e-02,  1.1607e-01,  3.0632e-01,\n",
      "          6.0837e-01, -3.5148e-01,  3.2126e-01, -1.2586e-01, -4.0780e-01],\n",
      "        [ 2.9655e-01,  2.9647e-01,  2.7662e-01,  5.4439e-01,  5.0016e-01,\n",
      "          3.4185e-01,  4.5677e-01,  8.0781e-02,  1.1607e-01,  3.0632e-01],\n",
      "        [-4.0780e-01,  4.7779e-01,  3.5817e-01,  1.9040e-01,  2.9280e-01,\n",
      "          7.3851e-01,  2.3082e-01, -4.1691e-02,  7.8916e-02, -1.0348e-02],\n",
      "        [ 4.9053e-02,  4.0565e-02, -1.0841e-01, -1.3797e-01, -1.0766e-01,\n",
      "         -5.3927e-03,  7.7431e-01,  2.1985e-01,  2.8543e-01, -1.1235e-01],\n",
      "        [ 3.6120e-01,  2.8410e-01,  1.9568e-01, -3.3746e-02,  6.5838e-01,\n",
      "          1.0908e-01,  1.6324e-01,  1.1754e-01,  2.8083e-01,  1.2565e-01],\n",
      "        [-3.1425e-01,  5.5316e-01, -1.2640e-02,  1.3172e-01,  4.0753e-02,\n",
      "          1.0000e+00, -5.3198e-02,  2.7269e-01,  3.0056e-01,  9.3680e-02],\n",
      "        [-1.2408e-01,  2.6153e-01,  4.9053e-02,  4.0565e-02, -1.0841e-01,\n",
      "         -1.3797e-01, -1.0766e-01, -5.3927e-03,  7.7431e-01,  2.1985e-01],\n",
      "        [-4.1691e-02,  7.8916e-02, -1.0348e-02,  1.5280e-01,  2.1893e-01,\n",
      "          3.3760e-01,  2.5729e-01,  4.2623e-01,  2.3686e-01,  8.3737e-02],\n",
      "        [ 1.7380e-01,  1.9597e-01,  3.5530e-01, -1.1406e-01,  1.9373e-01,\n",
      "          1.7058e-01,  5.3802e-01,  5.3602e-02, -1.9176e-03,  1.0056e-01],\n",
      "        [ 1.9597e-01,  3.5530e-01, -1.1406e-01,  1.9373e-01,  1.7058e-01,\n",
      "          5.3802e-01,  5.3602e-02, -1.9176e-03,  1.0056e-01,  2.5305e-02],\n",
      "        [ 2.4526e-01,  1.7931e-01, -2.8728e-01, -7.1925e-01, -1.8664e-01,\n",
      "          1.5064e-01,  3.3071e-01, -2.2750e-02, -1.3577e-01,  2.1304e-01],\n",
      "        [ 1.5542e-01,  8.0404e-01,  1.3721e-01,  1.8754e-01, -3.1425e-01,\n",
      "          5.5316e-01, -1.2640e-02,  1.3172e-01,  4.0753e-02,  1.0000e+00],\n",
      "        [-2.8728e-01, -7.1925e-01, -1.8664e-01,  1.5064e-01,  3.3071e-01,\n",
      "         -2.2750e-02, -1.3577e-01,  2.1304e-01, -2.8911e-01,  3.0196e-01],\n",
      "        [ 3.0841e-01,  5.6677e-01,  3.4962e-05,  8.7276e-03,  2.2271e-01,\n",
      "          5.5576e-01,  5.0649e-01,  2.6822e-01,  3.0171e-01,  4.1845e-01],\n",
      "        [ 1.9040e-01,  2.9280e-01,  7.3851e-01,  2.3082e-01, -4.1691e-02,\n",
      "          7.8916e-02, -1.0348e-02,  1.5280e-01,  2.1893e-01,  3.3760e-01],\n",
      "        [ 5.0016e-01,  3.4185e-01,  4.5677e-01,  8.0781e-02,  1.1607e-01,\n",
      "          3.0632e-01,  6.0837e-01, -3.5148e-01,  3.2126e-01, -1.2586e-01],\n",
      "        [ 2.7662e-01,  5.4439e-01,  5.0016e-01,  3.4185e-01,  4.5677e-01,\n",
      "          8.0781e-02,  1.1607e-01,  3.0632e-01,  6.0837e-01, -3.5148e-01],\n",
      "        [ 2.0556e-01,  1.1900e-01,  2.6385e-01,  1.4408e-01, -3.5142e-01,\n",
      "          3.4204e-01,  5.1369e-01,  6.7425e-01,  3.4752e-01, -1.9770e-01],\n",
      "        [ 1.0056e-01,  2.5305e-02,  2.6436e-01,  1.7100e-01,  2.6726e-01,\n",
      "          1.9065e-01, -3.5823e-03,  4.5258e-02,  2.3182e-01,  5.7255e-01],\n",
      "        [-1.3797e-01, -1.0766e-01, -5.3927e-03,  7.7431e-01,  2.1985e-01,\n",
      "          2.8543e-01, -1.1235e-01,  4.7067e-01,  1.5542e-01,  8.0404e-01],\n",
      "        [ 9.8393e-03,  4.3657e-02, -2.4374e-01,  5.1095e-01,  3.7219e-01,\n",
      "          1.1934e-02,  6.3143e-02,  1.6346e-01,  1.7941e-01,  6.2162e-01],\n",
      "        [ 1.9373e-01,  1.7058e-01,  5.3802e-01,  5.3602e-02, -1.9176e-03,\n",
      "          1.0056e-01,  2.5305e-02,  2.6436e-01,  1.7100e-01,  2.6726e-01],\n",
      "        [-6.7088e-02,  6.0358e-02,  1.5594e-01,  5.2960e-01,  3.0018e-01,\n",
      "          3.0841e-01,  5.6677e-01,  3.4962e-05,  8.7276e-03,  2.2271e-01],\n",
      "        [ 4.0753e-02,  1.0000e+00, -5.3198e-02,  2.7269e-01,  3.0056e-01,\n",
      "          9.3680e-02, -1.5271e-01,  1.8759e-01,  1.5732e-02, -8.9042e-02],\n",
      "        [ 8.0404e-01,  1.3721e-01,  1.8754e-01, -3.1425e-01,  5.5316e-01,\n",
      "         -1.2640e-02,  1.3172e-01,  4.0753e-02,  1.0000e+00, -5.3198e-02],\n",
      "        [ 4.5011e-01,  2.1339e-02,  1.1572e-01, -1.4797e-01,  5.1159e-01,\n",
      "          1.7978e-01, -6.6685e-02, -9.2879e-02,  3.2695e-01, -4.1936e-02],\n",
      "        [ 6.0837e-01, -3.5148e-01,  3.2126e-01, -1.2586e-01, -4.0780e-01,\n",
      "          4.7779e-01,  3.5817e-01,  1.9040e-01,  2.9280e-01,  7.3851e-01],\n",
      "        [ 1.6416e-02, -3.4039e-02, -7.6294e-01,  2.5993e-01,  9.8393e-03,\n",
      "          4.3657e-02, -2.4374e-01,  5.1095e-01,  3.7219e-01,  1.1934e-02],\n",
      "        [-1.9176e-03,  1.0056e-01,  2.5305e-02,  2.6436e-01,  1.7100e-01,\n",
      "          2.6726e-01,  1.9065e-01, -3.5823e-03,  4.5258e-02,  2.3182e-01],\n",
      "        [-1.9770e-01,  1.7380e-01,  1.9597e-01,  3.5530e-01, -1.1406e-01,\n",
      "          1.9373e-01,  1.7058e-01,  5.3802e-01,  5.3602e-02, -1.9176e-03],\n",
      "        [ 4.2623e-01,  2.3686e-01,  8.3737e-02, -7.6207e-01,  4.9140e-02,\n",
      "          4.5011e-01,  2.1339e-02,  1.1572e-01, -1.4797e-01,  5.1159e-01],\n",
      "        [ 2.8083e-01,  1.2565e-01,  6.7744e-03, -1.2281e-01,  2.4526e-01,\n",
      "          1.7931e-01, -2.8728e-01, -7.1925e-01, -1.8664e-01,  1.5064e-01],\n",
      "        [ 4.3170e-02,  1.4318e-02,  3.6120e-01,  2.8410e-01,  1.9568e-01,\n",
      "         -3.3746e-02,  6.5838e-01,  1.0908e-01,  1.6324e-01,  1.1754e-01],\n",
      "        [ 2.3082e-01, -4.1691e-02,  7.8916e-02, -1.0348e-02,  1.5280e-01,\n",
      "          2.1893e-01,  3.3760e-01,  2.5729e-01,  4.2623e-01,  2.3686e-01],\n",
      "        [-1.1235e-01,  4.7067e-01,  1.5542e-01,  8.0404e-01,  1.3721e-01,\n",
      "          1.8754e-01, -3.1425e-01,  5.5316e-01, -1.2640e-02,  1.3172e-01],\n",
      "        [-6.6685e-02, -9.2879e-02,  3.2695e-01, -4.1936e-02,  6.2240e-02,\n",
      "          8.4908e-02, -1.2408e-01,  2.6153e-01,  4.9053e-02,  4.0565e-02],\n",
      "        [ 5.1095e-01,  3.7219e-01,  1.1934e-02,  6.3143e-02,  1.6346e-01,\n",
      "          1.7941e-01,  6.2162e-01,  2.3668e-01,  9.4636e-03,  1.1482e-01],\n",
      "        [ 3.3760e-01,  2.5729e-01,  4.2623e-01,  2.3686e-01,  8.3737e-02,\n",
      "         -7.6207e-01,  4.9140e-02,  4.5011e-01,  2.1339e-02,  1.1572e-01],\n",
      "        [-1.2640e-02,  1.3172e-01,  4.0753e-02,  1.0000e+00, -5.3198e-02,\n",
      "          2.7269e-01,  3.0056e-01,  9.3680e-02, -1.5271e-01,  1.8759e-01],\n",
      "        [ 1.2565e-01,  6.7744e-03, -1.2281e-01,  2.4526e-01,  1.7931e-01,\n",
      "         -2.8728e-01, -7.1925e-01, -1.8664e-01,  1.5064e-01,  3.3071e-01],\n",
      "        [ 5.4439e-01,  5.0016e-01,  3.4185e-01,  4.5677e-01,  8.0781e-02,\n",
      "          1.1607e-01,  3.0632e-01,  6.0837e-01, -3.5148e-01,  3.2126e-01],\n",
      "        [-1.8451e-01,  1.2485e-01,  1.2047e-01,  2.5584e-01, -1.5509e-01,\n",
      "          4.0646e-01,  1.0382e-01,  2.4418e-01,  1.6416e-02, -3.4039e-02],\n",
      "        [ 1.3721e-01,  1.8754e-01, -3.1425e-01,  5.5316e-01, -1.2640e-02,\n",
      "          1.3172e-01,  4.0753e-02,  1.0000e+00, -5.3198e-02,  2.7269e-01],\n",
      "        [ 2.6153e-01,  4.9053e-02,  4.0565e-02, -1.0841e-01, -1.3797e-01,\n",
      "         -1.0766e-01, -5.3927e-03,  7.7431e-01,  2.1985e-01,  2.8543e-01],\n",
      "        [-7.6294e-01,  2.5993e-01,  9.8393e-03,  4.3657e-02, -2.4374e-01,\n",
      "          5.1095e-01,  3.7219e-01,  1.1934e-02,  6.3143e-02,  1.6346e-01],\n",
      "        [-8.9042e-02, -2.3675e-01,  2.2300e-01,  4.3170e-02,  1.4318e-02,\n",
      "          3.6120e-01,  2.8410e-01,  1.9568e-01, -3.3746e-02,  6.5838e-01],\n",
      "        [ 6.8463e-01,  2.0556e-01,  1.1900e-01,  2.6385e-01,  1.4408e-01,\n",
      "         -3.5142e-01,  3.4204e-01,  5.1369e-01,  6.7425e-01,  3.4752e-01],\n",
      "        [-1.0766e-01, -5.3927e-03,  7.7431e-01,  2.1985e-01,  2.8543e-01,\n",
      "         -1.1235e-01,  4.7067e-01,  1.5542e-01,  8.0404e-01,  1.3721e-01],\n",
      "        [ 3.2695e-01, -4.1936e-02,  6.2240e-02,  8.4908e-02, -1.2408e-01,\n",
      "          2.6153e-01,  4.9053e-02,  4.0565e-02, -1.0841e-01, -1.3797e-01],\n",
      "        [ 2.4418e-01,  1.6416e-02, -3.4039e-02, -7.6294e-01,  2.5993e-01,\n",
      "          9.8393e-03,  4.3657e-02, -2.4374e-01,  5.1095e-01,  3.7219e-01],\n",
      "        [ 8.7276e-03,  2.2271e-01,  5.5576e-01,  5.0649e-01,  2.6822e-01,\n",
      "          3.0171e-01,  4.1845e-01,  2.9655e-01,  2.9647e-01,  2.7662e-01],\n",
      "        [ 5.2960e-01,  3.0018e-01,  3.0841e-01,  5.6677e-01,  3.4962e-05,\n",
      "          8.7276e-03,  2.2271e-01,  5.5576e-01,  5.0649e-01,  2.6822e-01],\n",
      "        [ 1.6324e-01,  1.1754e-01,  2.8083e-01,  1.2565e-01,  6.7744e-03,\n",
      "         -1.2281e-01,  2.4526e-01,  1.7931e-01, -2.8728e-01, -7.1925e-01],\n",
      "        [ 4.9140e-02,  4.5011e-01,  2.1339e-02,  1.1572e-01, -1.4797e-01,\n",
      "          5.1159e-01,  1.7978e-01, -6.6685e-02, -9.2879e-02,  3.2695e-01],\n",
      "        [ 2.2271e-01,  5.5576e-01,  5.0649e-01,  2.6822e-01,  3.0171e-01,\n",
      "          4.1845e-01,  2.9655e-01,  2.9647e-01,  2.7662e-01,  5.4439e-01],\n",
      "        [ 3.5730e-01,  6.8463e-01,  2.0556e-01,  1.1900e-01,  2.6385e-01,\n",
      "          1.4408e-01, -3.5142e-01,  3.4204e-01,  5.1369e-01,  6.7425e-01],\n",
      "        [ 3.5817e-01,  1.9040e-01,  2.9280e-01,  7.3851e-01,  2.3082e-01,\n",
      "         -4.1691e-02,  7.8916e-02, -1.0348e-02,  1.5280e-01,  2.1893e-01],\n",
      "        [ 1.5594e-01,  5.2960e-01,  3.0018e-01,  3.0841e-01,  5.6677e-01,\n",
      "          3.4962e-05,  8.7276e-03,  2.2271e-01,  5.5576e-01,  5.0649e-01],\n",
      "        [-1.4797e-01,  5.1159e-01,  1.7978e-01, -6.6685e-02, -9.2879e-02,\n",
      "          3.2695e-01, -4.1936e-02,  6.2240e-02,  8.4908e-02, -1.2408e-01],\n",
      "        [ 1.1482e-01,  3.3131e-01, -6.7088e-02,  6.0358e-02,  1.5594e-01,\n",
      "          5.2960e-01,  3.0018e-01,  3.0841e-01,  5.6677e-01,  3.4962e-05]]), tensor([[ 0.1447],\n",
      "        [ 0.5065],\n",
      "        [ 0.4073],\n",
      "        [ 0.4778],\n",
      "        [ 0.6084],\n",
      "        [ 0.1528],\n",
      "        [ 0.4707],\n",
      "        [ 0.0068],\n",
      "        [-0.1527],\n",
      "        [ 0.2854],\n",
      "        [-0.7621],\n",
      "        [ 0.0253],\n",
      "        [ 0.2644],\n",
      "        [-0.2891],\n",
      "        [-0.0532],\n",
      "        [ 0.3573],\n",
      "        [ 0.2965],\n",
      "        [ 0.2573],\n",
      "        [-0.4078],\n",
      "        [ 0.3213],\n",
      "        [ 0.1738],\n",
      "        [ 0.2330],\n",
      "        [ 0.1372],\n",
      "        [ 0.2367],\n",
      "        [ 0.1906],\n",
      "        [ 0.5558],\n",
      "        [-0.2368],\n",
      "        [ 0.2727],\n",
      "        [ 0.0622],\n",
      "        [ 0.2308],\n",
      "        [ 0.0631],\n",
      "        [ 0.5726],\n",
      "        [ 0.1006],\n",
      "        [ 0.1798],\n",
      "        [ 0.3307],\n",
      "        [ 0.2808],\n",
      "        [ 0.0837],\n",
      "        [ 0.0408],\n",
      "        [-0.1084],\n",
      "        [ 0.3313],\n",
      "        [-0.1480],\n",
      "        [ 0.0157],\n",
      "        [-0.0227],\n",
      "        [-0.1259],\n",
      "        [-0.7629],\n",
      "        [ 0.3006],\n",
      "        [-0.1124],\n",
      "        [ 0.1794],\n",
      "        [ 0.1091],\n",
      "        [-0.1977],\n",
      "        [ 0.1875],\n",
      "        [-0.1077],\n",
      "        [ 0.0119],\n",
      "        [ 0.5444],\n",
      "        [ 0.3017],\n",
      "        [-0.1866],\n",
      "        [-0.0419],\n",
      "        [ 0.5002],\n",
      "        [ 0.3475],\n",
      "        [ 0.3376],\n",
      "        [ 0.2682],\n",
      "        [ 0.2615],\n",
      "        [ 0.0087]])]\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_loader):\n",
    "    print(i, item)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oli/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/oli/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for seq, labels in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(seq.unsqueeze(-1))  # Add an extra dimension for num_features\n",
    "    labels = labels.view(-1)  # Reshape labels to be 1D\n",
    "    single_loss = criterion(y_pred[:, -1], labels)  # Use the last value of each sequence for prediction\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7621733546257019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validate the model\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        y_test_pred = model(seq.unsqueeze(-1))  # Add an extra dimension for num_features\n",
    "        test_loss = criterion(y_test_pred[:, -1], labels)  # Use the last value of each sequence for prediction\n",
    "\n",
    "print(f'Test loss: {test_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11423748]\n",
      " [-0.09807914]\n",
      " [-0.10716513]\n",
      " [-0.1321582 ]\n",
      " [-0.08726153]\n",
      " [-0.09321956]\n",
      " [-0.12088523]\n",
      " [-0.09219545]\n",
      " [-0.11479194]\n",
      " [-0.09588676]\n",
      " [-0.09982554]\n",
      " [-0.12001492]\n",
      " [-0.10131675]\n",
      " [-0.10309248]\n",
      " [-0.1009479 ]\n",
      " [-0.12503815]\n",
      " [-0.10166435]\n",
      " [-0.09513269]\n",
      " [-0.11008015]\n",
      " [-0.10532748]\n",
      " [-0.11566451]\n",
      " [-0.09872688]\n",
      " [-0.0985194 ]\n",
      " [-0.11570175]\n",
      " [-0.10483246]\n",
      " [-0.10976406]\n",
      " [-0.09943962]\n",
      " [-0.09843545]\n",
      " [-0.12224382]\n",
      " [-0.11112357]\n",
      " [-0.10485278]\n",
      " [-0.1032197 ]\n",
      " [-0.10052475]\n",
      " [-0.10509852]\n",
      " [-0.10579433]\n",
      " [-0.09865479]\n",
      " [-0.10583702]\n",
      " [-0.10599229]\n",
      " [-0.10362068]\n",
      " [-0.09971827]\n",
      " [-0.10687425]\n",
      " [-0.10904612]\n",
      " [-0.10176252]\n",
      " [-0.10950609]\n",
      " [-0.0979264 ]\n",
      " [-0.11734486]\n",
      " [-0.10558103]\n",
      " [-0.10523278]]\n",
      "Predicted Value: [[-0.02841371]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        seq = seq.view(-1, 10, 1)  # Reshape your sequence data if necessary\n",
    "        y_pred_test = model(seq)\n",
    "        # Extract the last value of each sequence and reshape to 2D array\n",
    "        y_pred_test_last = y_pred_test[:, -1].numpy().reshape(-1, 1)\n",
    "        # Inverse transform the predictions\n",
    "        y_pred_test_inv = scaler.inverse_transform(y_pred_test_last)\n",
    "        predictions.extend(y_pred_test_inv)\n",
    "\n",
    "# Convert the list of predictions to a numpy array\n",
    "predictions = np.array(predictions)\n",
    "print(predictions)\n",
    "\n",
    "# To inverse transform the scaling on a single prediction value (e.g., the last prediction):\n",
    "predicted_value = scaler.inverse_transform(predictions[-1].reshape(-1, 1))\n",
    "print(f\"Predicted Value: {predicted_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.11423748], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "\n",
    "\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for the next time step: [156.30432]\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'predicted_log_returns' is a numpy array containing your log return predictions\n",
    "# and 'last_known_price' is the last known price from your test data\n",
    "last_known_price = df['Close'].iloc[-1]\n",
    "predicted_prices = [last_known_price]\n",
    "\n",
    "\n",
    "predicted_price = predicted_prices[-1] * np.exp(predictions[0])\n",
    "predicted_prices.append(predicted_price)\n",
    "\n",
    "# The predicted price for the next time step after the last sequence in your test data\n",
    "next_predicted_price = predicted_prices[-1]\n",
    "print(f\"Predicted price for the next time step: {next_predicted_price}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
