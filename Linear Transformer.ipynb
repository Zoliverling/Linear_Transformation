{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install torch\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Fourier_Transformer import LinearTransformer, create_inout_sequences\n",
    "from LSTM import LSTMModel, train_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('C:/Users/19495/Documents/GitHub/Linear_Transformation/stock_data/TSLA.csv') \n",
    "\n",
    "df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df.dropna(inplace=True)  # Remove NaNs\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df['log_return'] = scaler.fit_transform(df['log_return'].values.reshape(-1,1))\n",
    "data = torch.FloatTensor(df['log_return'].values).view(-1) # Convert the DataFrame to a PyTorch Tensor\n",
    "\n",
    "seq_length = 20  # Based on how many days you want to use to predict the next day\n",
    "inout_seq = create_inout_sequences(data, seq_length) # Create sequences\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(inout_seq) * 0.80)\n",
    "train_set = inout_seq[:train_size]\n",
    "test_set = inout_seq[train_size:]\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the model with 2 layers, 64 dimensions, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LinearTransformer(feature_size=1, num_layers=2, d_model=64, d_ff = 2048, num_heads=8)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([55, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training Loss: 2.5865, Test Loss: 0.5822\n",
      "Epoch 2/50 - Training Loss: 0.2760, Test Loss: 0.4178\n",
      "Epoch 3/50 - Training Loss: 0.4330, Test Loss: 0.2681\n",
      "Epoch 4/50 - Training Loss: 0.1820, Test Loss: 0.0868\n",
      "Epoch 5/50 - Training Loss: 0.0897, Test Loss: 0.2206\n",
      "Epoch 6/50 - Training Loss: 0.1690, Test Loss: 0.2077\n",
      "Epoch 7/50 - Training Loss: 0.1255, Test Loss: 0.1119\n",
      "Epoch 8/50 - Training Loss: 0.0760, Test Loss: 0.0838\n",
      "Epoch 9/50 - Training Loss: 0.0814, Test Loss: 0.0907\n",
      "Epoch 10/50 - Training Loss: 0.0910, Test Loss: 0.0888\n",
      "Epoch 11/50 - Training Loss: 0.0797, Test Loss: 0.0839\n",
      "Epoch 12/50 - Training Loss: 0.0729, Test Loss: 0.0887\n",
      "Epoch 13/50 - Training Loss: 0.0715, Test Loss: 0.0984\n",
      "Epoch 14/50 - Training Loss: 0.0743, Test Loss: 0.1004\n",
      "Epoch 15/50 - Training Loss: 0.0737, Test Loss: 0.0958\n",
      "Epoch 16/50 - Training Loss: 0.0717, Test Loss: 0.0879\n",
      "Epoch 17/50 - Training Loss: 0.0701, Test Loss: 0.0856\n",
      "Epoch 18/50 - Training Loss: 0.0707, Test Loss: 0.0848\n",
      "Epoch 19/50 - Training Loss: 0.0700, Test Loss: 0.0856\n",
      "Epoch 20/50 - Training Loss: 0.0698, Test Loss: 0.0875\n",
      "Epoch 21/50 - Training Loss: 0.0711, Test Loss: 0.0894\n",
      "Epoch 22/50 - Training Loss: 0.0697, Test Loss: 0.0898\n",
      "Epoch 23/50 - Training Loss: 0.0709, Test Loss: 0.0881\n",
      "Epoch 24/50 - Training Loss: 0.0692, Test Loss: 0.0869\n",
      "Epoch 25/50 - Training Loss: 0.0692, Test Loss: 0.0863\n",
      "Epoch 26/50 - Training Loss: 0.0697, Test Loss: 0.0866\n",
      "Epoch 27/50 - Training Loss: 0.0725, Test Loss: 0.0880\n",
      "Epoch 28/50 - Training Loss: 0.0704, Test Loss: 0.0887\n",
      "Epoch 29/50 - Training Loss: 0.0697, Test Loss: 0.0879\n",
      "Epoch 30/50 - Training Loss: 0.0689, Test Loss: 0.0877\n",
      "Epoch 31/50 - Training Loss: 0.0680, Test Loss: 0.0876\n",
      "Epoch 32/50 - Training Loss: 0.0711, Test Loss: 0.0877\n",
      "Epoch 33/50 - Training Loss: 0.0702, Test Loss: 0.0868\n",
      "Epoch 34/50 - Training Loss: 0.0699, Test Loss: 0.0868\n",
      "Epoch 35/50 - Training Loss: 0.0683, Test Loss: 0.0873\n",
      "Epoch 36/50 - Training Loss: 0.0707, Test Loss: 0.0878\n",
      "Epoch 37/50 - Training Loss: 0.0709, Test Loss: 0.0886\n",
      "Epoch 38/50 - Training Loss: 0.0695, Test Loss: 0.0884\n",
      "Epoch 39/50 - Training Loss: 0.0719, Test Loss: 0.0894\n",
      "Epoch 40/50 - Training Loss: 0.0700, Test Loss: 0.0882\n",
      "Epoch 41/50 - Training Loss: 0.0685, Test Loss: 0.0858\n",
      "Epoch 42/50 - Training Loss: 0.0716, Test Loss: 0.0857\n",
      "Epoch 43/50 - Training Loss: 0.0693, Test Loss: 0.0873\n",
      "Epoch 44/50 - Training Loss: 0.0708, Test Loss: 0.0901\n",
      "Epoch 45/50 - Training Loss: 0.0699, Test Loss: 0.0914\n",
      "Epoch 46/50 - Training Loss: 0.0710, Test Loss: 0.0880\n",
      "Epoch 47/50 - Training Loss: 0.0687, Test Loss: 0.0857\n",
      "Epoch 48/50 - Training Loss: 0.0688, Test Loss: 0.0855\n",
      "Epoch 49/50 - Training Loss: 0.0700, Test Loss: 0.0871\n",
      "Epoch 50/50 - Training Loss: 0.0681, Test Loss: 0.0893\n",
      "Overall Average Training Loss: 0.1387\n",
      "Overall Average Test Loss: 0.1138\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs = 50 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = []\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq.unsqueeze(-1))  # Adjust dimensions if necessary\n",
    "        labels = labels.view(-1)  # Ensure label dimensions match output\n",
    "        loss = criterion(y_pred[:, -1], labels)  # Assuming using last output for prediction\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    # Calculate and store the average training loss for this epoch\n",
    "    train_losses.append(np.mean(epoch_train_loss))\n",
    "\n",
    "    # Validation or Testing phase\n",
    "    model.eval()\n",
    "    epoch_test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for seq, labels in test_loader:\n",
    "            y_pred = model(seq.unsqueeze(-1))\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(y_pred[:, -1], labels)\n",
    "            epoch_test_loss.append(loss.item())\n",
    "    \n",
    "    # Calculate and store the average test loss for this epoch\n",
    "    test_losses.append(np.mean(epoch_test_loss))\n",
    "    \n",
    "    # Optional: print out loss information to monitor progress\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Training Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "\n",
    "overall_avg_train_loss = np.mean(train_losses)\n",
    "overall_avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "print(f'Overall Average Training Loss: {overall_avg_train_loss:.4f}')\n",
    "print(f'Overall Average Test Loss: {overall_avg_test_loss:.4f}')\n",
    "\n",
    "# Assuming 'model' is your model instance and it has been trained\n",
    "torch.save(model.state_dict(), 'transformer_fourier.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearTransformer(\n",
       "  (fourier_transform): FourierTransform()\n",
       "  (positional_encoder): Embedding(1000, 64)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x TransformerEncoderLayer(\n",
       "      (fourier_transform): FourierTransform()\n",
       "      (feed_forward): PositionwiseFeedforward(\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-1): 2 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedforward(\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = LinearTransformer(feature_size=1, num_layers=2, d_model=64, num_heads=8, d_ff=2048, dropout=0.1)\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load('transformer_fourier.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stock data of Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have new input data for prediction\n",
    "df = pd.read_csv('C:/Users/19495/Documents/GitHub/Linear_Transformation/stock_data/AAPL.csv') \n",
    "\n",
    "df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df.dropna(inplace=True)  # Remove NaNs\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df['log_return'] = scaler.fit_transform(df['log_return'].values.reshape(-1,1))\n",
    "new_input_data = df['log_return']  # This should be your new input data\n",
    "\n",
    "# Scale and preprocess your new data\n",
    "new_input_scaled = scaler.transform(np.array(new_input_data).reshape(-1, 1))\n",
    "\n",
    "# Convert to Tensor\n",
    "new_input_tensor = torch.FloatTensor(new_input_scaled).view(-1)\n",
    "\n",
    "# Create sequences (same sequence length as used during training)\n",
    "new_sequences = create_inout_sequences(new_input_tensor, seq_length)\n",
    "\n",
    "# Extract just the sequence part (ignoring labels if it's purely for prediction)\n",
    "new_sequences = [seq[0] for seq in new_sequences]  # Assuming you only need the sequences\n",
    "\n",
    "# Optionally, convert to a DataLoader if dealing with many sequences\n",
    "predict_loader = DataLoader(new_sequences, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for sequences in predict_loader:\n",
    "        sequences = sequences.view(sequences.shape[0], seq_length, 1)  # Reshape if necessary\n",
    "        output = model(sequences)\n",
    "        predicted_values = output[:, -1]  # If you're predicting the last value\n",
    "        predictions.extend(predicted_values.numpy())\n",
    "\n",
    "# Inverse transform the predictions if necessary\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for the next time step: [169.67157]\n"
     ]
    }
   ],
   "source": [
    "last_known_price = df['Close'].iloc[-1]\n",
    "predicted_prices = [last_known_price]\n",
    "\n",
    "\n",
    "predicted_price = predicted_prices[-1] * np.exp(predictions[0])\n",
    "predicted_prices.append(predicted_price)\n",
    "\n",
    "# The predicted price for the next time step after the last sequence in your test data\n",
    "next_predicted_price = predicted_prices[-1]\n",
    "print(f\"Predicted price for the next time step: {next_predicted_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence, label = self.sequences[index]\n",
    "        # Convert numpy arrays to torch tensors and ensure they are of type float32\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('C:/Users/19495/Documents/GitHub/Linear_Transformation/stock_data/TSLA.csv')\n",
    "\n",
    "# Normalize data \n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_normalized = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "# Define window size\n",
    "window_size = 10\n",
    "\n",
    "# Create sequences\n",
    "inout_seq = create_inout_sequences(data_normalized, window_size)\n",
    "\n",
    "# Split data into train and test\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(inout_seq) * split_ratio)\n",
    "train_seq = inout_seq[:split_index]\n",
    "test_seq = inout_seq[split_index:]\n",
    "\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_seq)\n",
    "test_dataset = TimeSeriesDataset(test_seq)\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    # Extract the device from the model's parameters\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Ensure data and target tensors are moved to the same device as the model\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.27838587760925293\n",
      "Epoch 2 Loss: 0.2244166135787964\n",
      "Epoch 3 Loss: 0.2143065482378006\n",
      "Epoch 4 Loss: 0.2776557207107544\n",
      "Epoch 5 Loss: 0.27410751581192017\n",
      "Epoch 6 Loss: 0.24769581854343414\n",
      "Epoch 7 Loss: 0.20124225318431854\n",
      "Epoch 8 Loss: 0.22182869911193848\n",
      "Epoch 9 Loss: 0.22796595096588135\n",
      "Epoch 10 Loss: 0.18641243875026703\n",
      "Average Loss: 0.5255939960479736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\19495\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([48, 1, 1])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMModel(input_dim=1, hidden_dim=50, num_layers=1, output_dim=1)\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "train_model(lstm_model, train_data, nn.MSELoss(), lstm_optimizer, num_epochs=10)\n",
    "evaluate_model(lstm_model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
